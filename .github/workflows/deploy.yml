name: CI/CD Pipeline - Ecommerce Platform

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  DOTNET_VERSION: "9.0"
  REGISTRY_URL: ${{ secrets.REGISTRY_URL }}
  REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME }}
  REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD }}
  IMAGE_CORE: moclaw/ecom-core-api
  IMAGE_USERS: moclaw/ecom-users-api
  KUBE_NAMESPACE: ecommerce
  CF_TUNNEL_HOST: ${{ secrets.CF_TUNNEL_HOST }}
  CF_TUNNEL_USER: ${{ secrets.CF_TUNNEL_USER }}
  KUBECTL_TIMEOUT: 300s
  HEALTH_CHECK_TIMEOUT: 180s

jobs:
  build-and-test:
    name: Build & Test
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      should-deploy: ${{ steps.changes.outputs.should-deploy }}

    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: üîç Check for changes
        id: changes
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should-deploy=true" >> $GITHUB_OUTPUT
            echo "Manual trigger - will deploy"
          elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "should-deploy=true" >> $GITHUB_OUTPUT
            echo "Main branch - will deploy"
          else
            if git diff --name-only HEAD~1 HEAD | grep -E "\\.(cs|csproj|sln|Dockerfile)$|^k8s/"; then
              echo "should-deploy=true" >> $GITHUB_OUTPUT
              echo "Source code changes detected - will deploy"
            else
              echo "should-deploy=false" >> $GITHUB_OUTPUT
              echo "No significant changes - skipping deployment"
            fi
          fi

      - name: üîß Setup .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}

      - name: üì¶ Cache NuGet packages
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj', '**/*.props', '**/*.targets') }}
          restore-keys: |
            ${{ runner.os }}-nuget-

      - name: üì¶ Restore dependencies
        run: |
          dotnet restore Moclaw.Ecommerce.sln --verbosity minimal

      - name: üèóÔ∏è Build solution
        run: |
          dotnet build Moclaw.Ecommerce.sln --configuration Release --no-restore --verbosity minimal

      - name: üß™ Run tests
        run: |
          if find . -name "*.Test*.csproj" -o -name "*Test.csproj" | head -1 | grep -q .; then
            echo "Running unit tests..."
            dotnet test --configuration Release --no-build --verbosity minimal --parallel
          else
            echo "No test projects found, skipping tests"
          fi

      - name: üè∑Ô∏è Generate metadata
        id: meta
        run: |
          TAG="${{ github.sha }}"
          SHORT_SHA="${TAG:0:8}"
          echo "tags=${SHORT_SHA}" >> $GITHUB_OUTPUT
  build-images:
    name: Build Images
    runs-on: ubuntu-latest
    needs: build-and-test
    if: needs.build-and-test.outputs.should-deploy == 'true'
    strategy:
      matrix:
        service:
          - name: core-api
            dockerfile: Ecom.Core/src/Ecom.Core.API/Dockerfile
            image_name: moclaw/ecom-core-api
          - name: users-api
            dockerfile: Ecom.Users/src/Ecom.Users.API/Dockerfile
            image_name: moclaw/ecom-users-api
      fail-fast: false
      max-parallel: 2

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üê≥ Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: üîê Login to Docker registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY_URL }}
          username: ${{ env.REGISTRY_USERNAME }}
          password: ${{ env.REGISTRY_PASSWORD }}

      - name: üèóÔ∏è Build and push ${{ matrix.service.name }}
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ${{ matrix.service.dockerfile }}
          push: true
          tags: |
            ${{ matrix.service.image_name }}:${{ needs.build-and-test.outputs.image-tag }}
            ${{ matrix.service.image_name }}:latest
          platforms: linux/amd64
          cache-from: type=registry,ref=${{ matrix.service.image_name }}:buildcache
          cache-to: type=registry,ref=${{ matrix.service.image_name }}:buildcache,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1
  deploy-minikube:
    name: Deploy via SSH
    runs-on: ubuntu-latest
    needs: [build-and-test, build-images]
    if: needs.build-and-test.outputs.should-deploy == 'true'

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîê Setup SSH
        shell: bash
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
          CF_TUNNEL_HOST: ${{ secrets.CF_TUNNEL_HOST }}
          CF_TUNNEL_USER: ${{ secrets.CF_TUNNEL_USER }}
        run: |
          # Create SSH directory if it doesn't exist
          mkdir -p ~/.ssh
          
          # Clean and prepare SSH key
          echo "$SSH_PRIVATE_KEY" | tr -d '\r' | sed '/^$/d' > deploy_key
          
          # Validate SSH key format
          if ! ssh-keygen -l -f deploy_key >/dev/null 2>&1; then
            echo "‚ùå Invalid SSH private key format"
            echo "Key preview (first 50 chars): $(head -c 50 deploy_key)..."
            exit 1
          fi
          
          chmod 600 deploy_key
          
          # Add host to known_hosts with retries
          echo "Adding host to known_hosts..."
          for i in {1..3}; do
            if ssh-keyscan -H -T 10 "$CF_TUNNEL_HOST" >> known_hosts 2>/dev/null; then
              echo "‚úÖ Host key added successfully"
              break
            fi
            echo "Attempt $i failed, retrying..."
            sleep 5
          done
          
          # Verify known_hosts file
          if [ ! -s known_hosts ]; then
            echo "‚ùå Failed to get host key, trying direct connection..."
            # Fallback: accept host key on first connection
            touch known_hosts
          fi
          
          # Test SSH connection with detailed output
          echo "Testing SSH connection..."
          if ssh -i deploy_key -o UserKnownHostsFile=known_hosts -o ConnectTimeout=30 -o ServerAliveInterval=10 -o StrictHostKeyChecking=no -v "$CF_TUNNEL_USER@$CF_TUNNEL_HOST" "echo 'SSH connection test successful'" 2>&1; then
            echo "‚úÖ SSH connection established successfully"
          else
            echo "‚ùå SSH connection failed, checking connectivity..."
            # Additional debugging
            echo "Checking host resolution..."
            nslookup "$CF_TUNNEL_HOST" || echo "DNS resolution failed"
            echo "Checking connectivity..."
            timeout 10 nc -zv "$CF_TUNNEL_HOST" 22 || echo "Port 22 not accessible"
            exit 1
          fi

      - name: üì§ Transfer deployment files
        env:
          CF_TUNNEL_HOST: ${{ secrets.CF_TUNNEL_HOST }}
          CF_TUNNEL_USER: ${{ secrets.CF_TUNNEL_USER }}
        run: |
          # Create deployment archive with better error handling
          echo "Creating deployment archive..."
          if [ -d "k8s" ]; then
            if [ -d ".github/scripts" ]; then
              tar -czf deployment.tar.gz k8s/ .github/scripts/
            else
              tar -czf deployment.tar.gz k8s/
            fi
          else
            echo "‚ùå No k8s directory found"
            exit 1
          fi
          
          # Verify archive was created
          if [ ! -f deployment.tar.gz ]; then
            echo "‚ùå Failed to create deployment archive"
            exit 1
          fi
          
          echo "Archive size: $(du -h deployment.tar.gz | cut -f1)"
          
          # Transfer with retry logic
          echo "Transferring deployment files..."
          for i in {1..3}; do
            if scp -i deploy_key -o UserKnownHostsFile=known_hosts -o StrictHostKeyChecking=no -o ConnectTimeout=30 deployment.tar.gz "$CF_TUNNEL_USER@$CF_TUNNEL_HOST":~/; then
              echo "‚úÖ File transfer successful"
              break
            else
              echo "Transfer attempt $i failed"
              if [ $i -eq 3 ]; then
                echo "‚ùå All transfer attempts failed"
                exit 1
              fi
              sleep 10
            fi
          done
          
          # Extract files on remote server with verification
          echo "Extracting files on remote server..."
          ssh -i deploy_key -o UserKnownHostsFile=known_hosts -o StrictHostKeyChecking=no -o ConnectTimeout=30 "$CF_TUNNEL_USER@$CF_TUNNEL_HOST" << 'EXTRACT_EOF'
            set -e
            echo "Cleaning previous deployment..."
            rm -rf ~/deployment
            mkdir -p ~/deployment
            
            echo "Extracting archive..."
            if tar -xzf ~/deployment.tar.gz -C ~/deployment; then
              echo "‚úÖ Extraction successful"
              rm ~/deployment.tar.gz
              echo "Deployment directory contents:"
              ls -la ~/deployment/
            else
              echo "‚ùå Extraction failed"
              exit 1
            fi
          EXTRACT_EOF

      - name: üöÄ Deploy to Minikube (Remote)
        env:
          CF_TUNNEL_HOST: ${{ secrets.CF_TUNNEL_HOST }}
          CF_TUNNEL_USER: ${{ secrets.CF_TUNNEL_USER }}
          REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD }}
          REGISTRY_URL: ${{ secrets.REGISTRY_URL }}
          REGISTRY_USERNAME: ${{ secrets.REGISTRY_USERNAME }}
          IMAGE_TAG: ${{ needs.build-and-test.outputs.image-tag }}
        run: |
          # Execute deployment with better error handling and connection management
          ssh -i deploy_key -o UserKnownHostsFile=known_hosts -o StrictHostKeyChecking=no -o ConnectTimeout=60 -o ServerAliveInterval=30 "$CF_TUNNEL_USER@$CF_TUNNEL_HOST" << 'DEPLOY_EOF'
          set -e
          
          # Set error handling
          trap 'echo "‚ùå Deployment failed at line $LINENO"' ERR
          
          # Export variables for remote execution with validation
          export CORE_IMAGE="${{ env.IMAGE_CORE }}:${{ env.IMAGE_TAG }}"
          export USERS_IMAGE="${{ env.IMAGE_USERS }}:${{ env.IMAGE_TAG }}"
          export KUBE_NAMESPACE="${{ env.KUBE_NAMESPACE }}"
          export KUBECTL_TIMEOUT="${{ env.KUBECTL_TIMEOUT }}"
          export REGISTRY_URL="${{ env.REGISTRY_URL }}"
          export REGISTRY_USERNAME="${{ env.REGISTRY_USERNAME }}"
          export REGISTRY_PASSWORD="${{ env.REGISTRY_PASSWORD }}"
          
          # Validate required variables
          for var in CORE_IMAGE USERS_IMAGE KUBE_NAMESPACE KUBECTL_TIMEOUT REGISTRY_URL REGISTRY_USERNAME REGISTRY_PASSWORD; do
            if [ -z "${!var}" ]; then
              echo "‚ùå Required variable $var is not set"
              exit 1
            fi
          done

          echo "üöÄ Starting deployment on remote server..."
          echo "Core Image: $CORE_IMAGE"
          echo "Users Image: $USERS_IMAGE"
          echo "Namespace: $KUBE_NAMESPACE"

          # Function for retry logic
          retry_command() {
            local max_attempts=3
            local attempt=1
            local command="$1"
            
            while [ $attempt -le $max_attempts ]; do
              echo "Attempt $attempt/$max_attempts: $command"
              if eval "$command"; then
                return 0
              fi
              attempt=$((attempt + 1))
              [ $attempt -le $max_attempts ] && sleep 10
            done
            echo "Command failed after $max_attempts attempts: $command"
            return 1
          }

          # Check/Start Minikube with retry
          echo "Checking Minikube status..."
          if ! (minikube status 2>/dev/null | grep -q "host: Running" && kubectl cluster-info 2>/dev/null); then
            echo "Starting Minikube cluster..."
            minikube delete 2>/dev/null || true
            retry_command "minikube start --driver=docker --kubernetes-version=v1.28.3 --memory=4096 --cpus=2 --disk-size=15g --wait=all"
            minikube addons enable ingress
            kubectl wait --for=condition=Ready nodes --all --timeout=180s
          fi

          MINIKUBE_IP=$(minikube ip)
          echo "Minikube IP: $MINIKUBE_IP"

          # Login to Docker registry
          echo "Logging into Docker registry..."
          echo "$REGISTRY_PASSWORD" | docker login $REGISTRY_URL -u "$REGISTRY_USERNAME" --password-stdin

          # Optimized image handling
          echo "Managing Docker images..."

          # Check if images exist locally first
          if ! docker image inspect "$CORE_IMAGE" >/dev/null 2>&1; then
            echo "Pulling Core API image..."
            retry_command "docker pull $CORE_IMAGE"
          fi

          if ! docker image inspect "$USERS_IMAGE" >/dev/null 2>&1; then
            echo "Pulling Users API image..."
            retry_command "docker pull $USERS_IMAGE"
          fi

          # Load images to minikube efficiently
          if ! minikube image ls | grep -q "${{ env.IMAGE_TAG }}"; then
            echo "Loading images to Minikube..."
            minikube image load "$CORE_IMAGE" &
            minikube image load "$USERS_IMAGE" &
            wait
            echo "‚úÖ Images loaded to Minikube"
          else
            echo "‚úÖ Images already present in Minikube"
          fi

          # Deploy Kubernetes resources with better error handling
          echo "Deploying Kubernetes resources..."
          cd ~/deployment

          # Create namespace
          kubectl create namespace $KUBE_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

          # Apply configurations
          [ -f k8s/secrets.yaml ] && kubectl apply -f k8s/secrets.yaml -n $KUBE_NAMESPACE
          [ -f k8s/configmap.yaml ] && kubectl apply -f k8s/configmap.yaml -n $KUBE_NAMESPACE

          # Deploy PostgreSQL if not exists
          if ! kubectl get deployment postgres -n $KUBE_NAMESPACE >/dev/null 2>&1; then
            [ -f k8s/postgres/deployment.yaml ] && kubectl apply -f k8s/postgres/deployment.yaml -n $KUBE_NAMESPACE
          fi

          # Deploy APIs
          [ -d k8s/core-api ] && kubectl apply -f k8s/core-api/ -n $KUBE_NAMESPACE
          [ -d k8s/users-api ] && kubectl apply -f k8s/users-api/ -n $KUBE_NAMESPACE

          # Update images
          kubectl set image deployment/core-api core-api=$CORE_IMAGE -n $KUBE_NAMESPACE --record
          kubectl set image deployment/users-api users-api=$USERS_IMAGE -n $KUBE_NAMESPACE --record

          # Apply environment patches
          kubectl patch deployment core-api -n $KUBE_NAMESPACE --type='merge' -p='{
            "spec": {
              "template": {
                "spec": {
                  "containers": [{
                    "name": "core-api",
                    "env": [
                      {"name": "ASPNETCORE_ENVIRONMENT", "value": "Production"},
                      {"name": "ASPNETCORE_URLS", "value": "http://+:8080"}
                    ]
                  }]
                }
              }
            }
          }' 2>/dev/null || echo "Core API patch already applied"

          kubectl patch deployment users-api -n $KUBE_NAMESPACE --type='merge' -p='{
            "spec": {
              "template": {
                "spec": {
                  "containers": [{
                    "name": "users-api",
                    "env": [
                      {"name": "ASPNETCORE_ENVIRONMENT", "value": "Production"},
                      {"name": "ASPNETCORE_URLS", "value": "http://+:8080"}
                    ]
                  }]
                }
              }
            }
          }' 2>/dev/null || echo "Users API patch already applied"

          # Wait for deployments with improved monitoring
          echo "Waiting for deployments to be ready..."

          # Wait for PostgreSQL first
          if kubectl get deployment postgres -n $KUBE_NAMESPACE >/dev/null 2>&1; then
            if ! kubectl get pods -l app=postgres -n $KUBE_NAMESPACE -o jsonpath='{.items[0].status.phase}' 2>/dev/null | grep -q "Running"; then
              kubectl rollout status deployment/postgres -n $KUBE_NAMESPACE --timeout=$KUBECTL_TIMEOUT
            fi
          fi

          # Wait for APIs in parallel
          kubectl rollout status deployment/core-api -n $KUBE_NAMESPACE --timeout=$KUBECTL_TIMEOUT &
          CORE_PID=$!
          kubectl rollout status deployment/users-api -n $KUBE_NAMESPACE --timeout=$KUBECTL_TIMEOUT &
          USERS_PID=$!

          # Wait for both deployments
          wait $CORE_PID && echo "‚úÖ Core API deployment ready" || echo "‚ùå Core API deployment failed"
          wait $USERS_PID && echo "‚úÖ Users API deployment ready" || echo "‚ùå Users API deployment failed"

          echo "‚úÖ All deployments processed!"

          # Enhanced health checks with proper error handling
          echo "Performing health checks..."

          health_check() {
            local url=$1
            local service_name=$2
            local max_attempts=15
            local attempt=1
            
            while [ $attempt -le $max_attempts ]; do
              if curl -s --connect-timeout 5 --max-time 10 "$url" >/dev/null 2>&1; then
                echo "‚úÖ $service_name is healthy"
                return 0
              fi
              echo "Waiting for $service_name... (attempt $attempt/$max_attempts)"
              sleep 10
              attempt=$((attempt + 1))
            done
            echo "‚ö†Ô∏è $service_name health check timeout"
            return 1
          }

          health_check "http://$MINIKUBE_IP:30301/" "Core API" &
          health_check "http://$MINIKUBE_IP:30302/" "Users API" &
          wait

          # Final status report
          echo ""
          echo "üöÄ Deployment Complete!"
          echo "=================================="
          echo "Core API: http://$MINIKUBE_IP:30301"
          echo "Users API: http://$MINIKUBE_IP:30302"
          echo "Image Tag: ${{ env.IMAGE_TAG }}"
          echo ""
          echo "Pod Status:"
          kubectl get pods -n $KUBE_NAMESPACE --no-headers | awk '{print "  " $1": "$3}'

          # Cleanup old images to save space
          docker image prune -f --filter "until=24h" >/dev/null 2>&1 || true

          EOF

      - name: üßπ Cleanup SSH
        if: always()
        run: |
          rm -f deploy_key known_hosts

      - name: üßπ Remote cleanup on failure
        if: failure()
        env:
          CF_TUNNEL_HOST: ${{ secrets.CF_TUNNEL_HOST }}
          CF_TUNNEL_USER: ${{ secrets.CF_TUNNEL_USER }}
          KUBE_NAMESPACE: ${{ env.KUBE_NAMESPACE }}
        run: |
          if [ -f deploy_key ]; then
            ssh -i deploy_key -o UserKnownHostsFile=known_hosts -o StrictHostKeyChecking=no "$CF_TUNNEL_USER@$CF_TUNNEL_HOST" '
              kubectl delete pods --field-selector=status.phase=Failed -n '"$KUBE_NAMESPACE"' --ignore-not-found=true
              docker system prune -f --filter "until=1h" >/dev/null 2>&1 || true
              rm -rf ~/deployment
            ' || echo "Remote cleanup failed"
          fi
