name: Ecommerce CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  DOCKER_REGISTRY: localhost:5000
  KUBECTL_VERSION: v1.28.0
  MONITORING_NAMESPACE: monitoring

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: '9.0.x'

    - name: Cache .NET dependencies
      uses: actions/cache@v3
      with:
        path: ~/.nuget/packages
        key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj') }}
        restore-keys: |
          ${{ runner.os }}-nuget-

    - name: Start Minikube
      uses: medyagh/setup-minikube@master
      with:
        minikube-version: 1.32.0
        kubernetes-version: v1.28.0
        driver: docker
        addons: |
          registry
          ingress
          metrics-server

    - name: Setup Multi-Node Cluster
      run: |
        # Add worker nodes for better pod distribution
        minikube node add --worker
        minikube node add --worker
        
        # Wait for all nodes to be ready
        kubectl wait --for=condition=Ready nodes --all --timeout=600s

    - name: Label Nodes for Service Distribution
      run: |
        # Get all node names
        NODES=($(kubectl get nodes -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | sort))
        
        echo "Available nodes: ${NODES[@]}"
        
        # Label nodes for specific services to ensure pod distribution
        if [ ${#NODES[@]} -ge 3 ]; then
          kubectl label nodes ${NODES[0]} service=core nodepool=core --overwrite
          kubectl label nodes ${NODES[1]} service=users nodepool=users --overwrite
          kubectl label nodes ${NODES[2]} service=monitoring nodepool=monitoring --overwrite
          echo "Multi-node setup: Core on ${NODES[0]}, Users on ${NODES[1]}, Monitoring on ${NODES[2]}"
        elif [ ${#NODES[@]} -eq 2 ]; then
          kubectl label nodes ${NODES[0]} service=core nodepool=core --overwrite
          kubectl label nodes ${NODES[1]} service=users nodepool=users --overwrite
          kubectl label nodes ${NODES[1]} service=monitoring nodepool=monitoring --overwrite
          echo "Two-node setup: Core on ${NODES[0]}, Users and Monitoring on ${NODES[1]}"
        else
          kubectl label nodes ${NODES[0]} service=core service=users service=monitoring nodepool=all --overwrite
          echo "Single-node setup: All services on ${NODES[0]}"
        fi
        
        kubectl get nodes --show-labels

    - name: Setup Monitoring Infrastructure
      run: |
        # Create monitoring namespace if it doesn't exist
        kubectl create namespace $MONITORING_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -
        
        # Check if Grafana is already deployed
        if ! kubectl get deployment grafana -n $MONITORING_NAMESPACE &>/dev/null; then
          echo "Setting up Grafana and Prometheus..."
          
          # Create Prometheus ConfigMap
          cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: prometheus-config
          namespace: $MONITORING_NAMESPACE
        data:
          prometheus.yml: |
            global:
              scrape_interval: 15s
            scrape_configs:
              - job_name: 'ecom-core-api'
                static_configs:
                  - targets: ['ecom-core-service.ecommerce.svc.cluster.local:80']
                metrics_path: '/metrics'
                scrape_interval: 10s
              - job_name: 'ecom-users-api'
                static_configs:
                  - targets: ['ecom-users-service.ecommerce.svc.cluster.local:80']
                metrics_path: '/metrics'
                scrape_interval: 10s
              - job_name: 'kubernetes-pods'
                kubernetes_sd_configs:
                  - role: pod
                    namespaces:
                      names:
                        - ecommerce
        EOF
          
          # Deploy Prometheus
          cat <<EOF | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: prometheus
          namespace: $MONITORING_NAMESPACE
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: prometheus
          template:
            metadata:
              labels:
                app: prometheus
            spec:
              nodeSelector:
                nodepool: monitoring
              containers:
              - name: prometheus
                image: prom/prometheus:latest
                ports:
                - containerPort: 9090
                volumeMounts:
                - name: config
                  mountPath: /etc/prometheus
                resources:
                  requests:
                    memory: "256Mi"
                    cpu: "250m"
                  limits:
                    memory: "512Mi"
                    cpu: "500m"
              volumes:
              - name: config
                configMap:
                  name: prometheus-config
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: prometheus
          namespace: $MONITORING_NAMESPACE
        spec:
          selector:
            app: prometheus
          ports:
          - port: 9090
            targetPort: 9090
        EOF
        
          # Deploy Grafana
          cat <<EOF | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: grafana
          namespace: $MONITORING_NAMESPACE
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: grafana
          template:
            metadata:
              labels:
                app: grafana
            spec:
              nodeSelector:
                nodepool: monitoring
              containers:
              - name: grafana
                image: grafana/grafana:latest
                ports:
                - containerPort: 3000
                env:
                - name: GF_SECURITY_ADMIN_PASSWORD
                  value: "admin123"
                resources:
                  requests:
                    memory: "256Mi"
                    cpu: "250m"
                  limits:
                    memory: "512Mi"
                    cpu: "500m"
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: grafana
          namespace: $MONITORING_NAMESPACE
        spec:
          selector:
            app: grafana
          ports:
          - port: 3000
            targetPort: 3000
          type: NodePort
        EOF
        else
          echo "Grafana already exists, skipping setup"
        fi

    - name: Run Tests
      run: |
        # Test Core service
        dotnet test Ecom.Core/test/EcomCore.Application.UnitTests/EcomCore.Application.UnitTests.csproj --configuration Release --logger trx --collect:"XPlat Code Coverage"
        dotnet test Ecom.Core/test/EcomCore.Domain.UnitTests/EcomCore.Domain.UnitTests.csproj --configuration Release --logger trx --collect:"XPlat Code Coverage"
        dotnet test Ecom.Core/test/EcomCore.Infrastructure.UnitTests/EcomCore.Infrastructure.UnitTests.csproj --configuration Release --logger trx --collect:"XPlat Code Coverage"
        
        # Test Users service
        dotnet test Ecom.Users/test/Ecom.Users.Application.UnitTests/Ecom.Users.Application.UnitTests.csproj --configuration Release --logger trx --collect:"XPlat Code Coverage"
        dotnet test Ecom.Users/test/Ecom.Users.Domain.UnitTests/Ecom.Users.Domain.UnitTests.csproj --configuration Release --logger trx --collect:"XPlat Code Coverage"
        dotnet test Ecom.Users/test/Ecom.Users.Infrastructure.UnitTests/Ecom.Users.Infrastructure.UnitTests.csproj --configuration Release --logger trx --collect:"XPlat Code Coverage"

    - name: Build Docker Images with Monitoring
      run: |
        # Wait for registry to be ready
        echo "Waiting for registry to be available..."
        kubectl wait --for=condition=ready pod -l kubernetes.io/minikube-addons=registry -n kube-system --timeout=300s
        
        # Get registry service details
        REGISTRY_SERVICE=$(kubectl get svc -n kube-system -l kubernetes.io/minikube-addons=registry -o jsonpath='{.items[0].metadata.name}')
        echo "Registry service: $REGISTRY_SERVICE"
        
        # Set up registry port-forward with proper cleanup
        kubectl port-forward -n kube-system svc/$REGISTRY_SERVICE 5000:80 &
        REGISTRY_PF_PID=$!
        echo "Registry port-forward PID: $REGISTRY_PF_PID"
        
        # Wait for port-forward to be ready
        sleep 15
        
        # Test registry connectivity
        curl -f http://localhost:5000/v2/ || echo "Registry not ready yet, waiting more..."
        sleep 10
        
        # Configure Docker for insecure registry
        sudo mkdir -p /etc/docker
        echo '{"insecure-registries":["localhost:5000"]}' | sudo tee /etc/docker/daemon.json
        sudo systemctl restart docker
        sleep 10
        
        # Build images with monitoring capabilities
        echo "Building Core API image with monitoring..."
        docker build -t ecom-core-api:latest -f Ecom.Core/src/Ecom.Core.API/Dockerfile \
          --build-arg ENABLE_METRICS=true .
        docker tag ecom-core-api:latest localhost:5000/ecom-core-api:latest
        
        echo "Building Users API image with monitoring..."
        docker build -t ecom-users-api:latest -f Ecom.Users/src/Ecom.Users.API/Dockerfile \
          --build-arg ENABLE_METRICS=true .
        docker tag ecom-users-api:latest localhost:5000/ecom-users-api:latest
        
        # Push images to registry with retry
        echo "Pushing Core API image..."
        for i in {1..3}; do
          if docker push localhost:5000/ecom-core-api:latest; then
            echo "Core API image pushed successfully"
            break
          else
            echo "Push attempt $i failed, retrying..."
            sleep 5
          fi
        done
        
        echo "Pushing Users API image..."
        for i in {1..3}; do
          if docker push localhost:5000/ecom-users-api:latest; then
            echo "Users API image pushed successfully"
            break
          else
            echo "Push attempt $i failed, retrying..."
            sleep 5
          fi
        done
        
        # Verify images in registry
        echo "Verifying images in registry:"
        curl -s http://localhost:5000/v2/_catalog | jq .
        
        # Store PID for cleanup
        echo "REGISTRY_PF_PID=$REGISTRY_PF_PID" >> $GITHUB_ENV

    - name: Create Kubernetes Resources
      run: |
        kubectl apply -f k8s/namespace.yml
        
        # Create monitoring service monitors
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Service
        metadata:
          name: ecom-core-metrics
          namespace: ecommerce
          labels:
            app: ecom-core-api
            monitoring: enabled
        spec:
          selector:
            app: ecom-core-api
          ports:
          - name: metrics
            port: 80
            targetPort: 8080
            protocol: TCP
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: ecom-users-metrics
          namespace: ecommerce
          labels:
            app: ecom-users-api
            monitoring: enabled
        spec:
          selector:
            app: ecom-users-api
          ports:
          - name: metrics
            port: 80
            targetPort: 8080
            protocol: TCP
        EOF

    - name: Deploy Core Service with Pod Affinity
      run: |
        # Update Core deployment with node affinity and resource limits
        cat <<EOF | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: ecom-core-api
          namespace: ecommerce
          labels:
            app: ecom-core-api
            version: latest
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: ecom-core-api
          template:
            metadata:
              labels:
                app: ecom-core-api
                version: latest
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "8080"
                prometheus.io/path: "/metrics"
            spec:
              nodeSelector:
                service: core
              affinity:
                podAntiAffinity:
                  preferredDuringSchedulingIgnoredDuringExecution:
                  - weight: 100
                    podAffinityTerm:
                      labelSelector:
                        matchExpressions:
                        - key: app
                          operator: In
                          values:
                          - ecom-users-api
                      topologyKey: kubernetes.io/hostname
              containers:
              - name: ecom-core-api
                image: localhost:5000/ecom-core-api:latest
                ports:
                - containerPort: 8080
                  name: http
                - containerPort: 8080
                  name: metrics
                resources:
                  requests:
                    memory: "512Mi"
                    cpu: "500m"
                  limits:
                    memory: "1Gi"
                    cpu: "1000m"
                livenessProbe:
                  httpGet:
                    path: /health
                    port: 8080
                  initialDelaySeconds: 30
                  periodSeconds: 10
                readinessProbe:
                  httpGet:
                    path: /health/ready
                    port: 8080
                  initialDelaySeconds: 5
                  periodSeconds: 5
                env:
                - name: ASPNETCORE_ENVIRONMENT
                  value: "Production"
                - name: ENABLE_METRICS
                  value: "true"
        EOF
        
        kubectl apply -f k8s/core-service/
        kubectl wait --for=condition=available --timeout=300s deployment/ecom-core-api -n ecommerce

    - name: Deploy Users Service with Pod Affinity
      run: |
        # Update Users deployment with node affinity and resource limits
        cat <<EOF | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: ecom-users-api
          namespace: ecommerce
          labels:
            app: ecom-users-api
            version: latest
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: ecom-users-api
          template:
            metadata:
              labels:
                app: ecom-users-api
                version: latest
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "8080"
                prometheus.io/path: "/metrics"
            spec:
              nodeSelector:
                service: users
              affinity:
                podAntiAffinity:
                  preferredDuringSchedulingIgnoredDuringExecution:
                  - weight: 100
                    podAffinityTerm:
                      labelSelector:
                        matchExpressions:
                        - key: app
                          operator: In
                          values:
                          - ecom-core-api
                      topologyKey: kubernetes.io/hostname
              containers:
              - name: ecom-users-api
                image: localhost:5000/ecom-users-api:latest
                ports:
                - containerPort: 8080
                  name: http
                - containerPort: 8080
                  name: metrics
                resources:
                  requests:
                    memory: "512Mi"
                    cpu: "500m"
                  limits:
                    memory: "1Gi"
                    cpu: "1000m"
                livenessProbe:
                  httpGet:
                    path: /health
                    port: 8080
                  initialDelaySeconds: 30
                  periodSeconds: 10
                readinessProbe:
                  httpGet:
                    path: /health/ready
                    port: 8080
                  initialDelaySeconds: 5
                  periodSeconds: 5
                env:
                - name: ASPNETCORE_ENVIRONMENT
                  value: "Production"
                - name: ENABLE_METRICS
                  value: "true"
        EOF
        
        kubectl apply -f k8s/users-service/
        kubectl wait --for=condition=available --timeout=300s deployment/ecom-users-api -n ecommerce

    - name: Verify Pod Distribution and Monitoring
      run: |
        echo "=== Pod Distribution Across Nodes ==="
        kubectl get pods -n ecommerce -o wide
        
        echo "=== Services ==="
        kubectl get services -n ecommerce
        
        echo "=== Monitoring Setup ==="
        kubectl get pods -n $MONITORING_NAMESPACE -o wide
        kubectl get services -n $MONITORING_NAMESPACE
        
        echo "=== Verifying Pod Isolation ==="
        CORE_NODE=$(kubectl get pod -n ecommerce -l app=ecom-core-api -o jsonpath='{.items[0].spec.nodeName}')
        USERS_NODE=$(kubectl get pod -n ecommerce -l app=ecom-users-api -o jsonpath='{.items[0].spec.nodeName}')
        
        echo "Core API Pod is on node: $CORE_NODE"
        echo "Users API Pod is on node: $USERS_NODE"
        
        if [ "$CORE_NODE" != "$USERS_NODE" ]; then
          echo "✅ Services are distributed across different nodes"
        else
          echo "⚠️ Services are on the same node (may be expected in single-node setup)"
        fi

    - name: Configure Grafana Datasource and Dashboard
      run: |
        # Wait for Grafana to be ready
        kubectl wait --for=condition=ready pod -l app=grafana -n $MONITORING_NAMESPACE --timeout=300s
        
        # Port forward to Grafana
        kubectl port-forward -n $MONITORING_NAMESPACE service/grafana 3000:3000 &
        GRAFANA_PF_PID=$!
        sleep 15
        
        # Configure Prometheus datasource
        curl -X POST \
          -H "Content-Type: application/json" \
          -d '{
            "name": "Prometheus",
            "type": "prometheus",
            "url": "http://prometheus.monitoring.svc.cluster.local:9090",
            "access": "proxy",
            "isDefault": true
          }' \
          http://admin:admin123@localhost:3000/api/datasources || echo "Datasource might already exist"
        
        # Import basic dashboard for microservices
        curl -X POST \
          -H "Content-Type: application/json" \
          -d '{
            "dashboard": {
              "title": "Ecommerce Services Dashboard",
              "panels": [
                {
                  "title": "Core API Response Time",
                  "type": "graph",
                  "targets": [{"expr": "http_request_duration_seconds{job=\"ecom-core-api\"}"}]
                },
                {
                  "title": "Users API Response Time", 
                  "type": "graph",
                  "targets": [{"expr": "http_request_duration_seconds{job=\"ecom-users-api\"}"}]
                }
              ]
            },
            "overwrite": true
          }' \
          http://admin:admin123@localhost:3000/api/dashboards/db || echo "Dashboard creation might have failed"
        
        kill $GRAFANA_PF_PID

    - name: Health Check with Monitoring
      run: |
        # Wait for pods to be ready
        kubectl wait --for=condition=ready pod -l app=ecom-core-api -n ecommerce --timeout=300s
        kubectl wait --for=condition=ready pod -l app=ecom-users-api -n ecommerce --timeout=300s
        
        # Get Minikube IP for testing
        MINIKUBE_IP=$(minikube ip)
        echo "Minikube IP: $MINIKUBE_IP"
        
        # Test Core service
        echo "Testing Core service..."
        kubectl port-forward -n ecommerce service/ecom-core-service 8080:80 &
        CORE_PF_PID=$!
        sleep 10
        
        # Simple health check for Core
        curl -f http://localhost:8080/health || echo "Core service health check failed"
        kill $CORE_PF_PID
        
        # Test Users service
        echo "Testing Users service..."
        kubectl port-forward -n ecommerce service/ecom-users-service 8081:80 &
        USERS_PF_PID=$!
        sleep 10
        
        # Simple health check for Users
        curl -f http://localhost:8081/health || echo "Users service health check failed"
        kill $USERS_PF_PID
        
        # Test monitoring endpoints
        echo "Testing monitoring endpoints..."
        kubectl port-forward -n ecommerce service/ecom-core-service 8080:80 &
        CORE_PF_PID=$!
        sleep 10
        
        curl -f http://localhost:8080/metrics || echo "Core metrics endpoint failed"
        curl -f http://localhost:8080/health || echo "Core health endpoint failed"
        kill $CORE_PF_PID
        
        kubectl port-forward -n ecommerce service/ecom-users-service 8081:80 &
        USERS_PF_PID=$!
        sleep 10
        
        curl -f http://localhost:8081/metrics || echo "Users metrics endpoint failed"
        curl -f http://localhost:8081/health || echo "Users health endpoint failed"
        kill $USERS_PF_PID

    - name: Generate Deployment Summary
      if: always()
      run: |
        echo "## Ecommerce Microservices Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "### Services Deployed (Each in Separate Pods):" >> $GITHUB_STEP_SUMMARY
        
        # Pod distribution info
        kubectl get pods -n ecommerce -o wide --no-headers | while read line; do
          echo "- $line" >> $GITHUB_STEP_SUMMARY
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Monitoring Stack:" >> $GITHUB_STEP_SUMMARY
        echo "- Prometheus: http://prometheus.monitoring.svc.cluster.local:9090" >> $GITHUB_STEP_SUMMARY
        echo "- Grafana: http://grafana.monitoring.svc.cluster.local:3000 (admin/admin123)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Service URLs:" >> $GITHUB_STEP_SUMMARY
        echo "- Core API: http://core.ecommerce.local" >> $GITHUB_STEP_SUMMARY
        echo "- Users API: http://users.ecommerce.local" >> $GITHUB_STEP_SUMMARY
        echo "- Metrics: /metrics endpoint on each service" >> $GITHUB_STEP_SUMMARY

  cleanup:
    runs-on: ubuntu-latest
    needs: build-and-deploy
    if: always()
    
    steps:
    - name: Cleanup Minikube
      if: github.event_name == 'pull_request'
      run: |
        minikube delete || true
